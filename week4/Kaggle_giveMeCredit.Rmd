---
title: "Give Me Some Credit Data Set from a Kaggle competition"
author: ""
date: ''
output: 
    pdf_document:
        number_sections: true
        includes:
            in_header: mystyles.sty
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
options(digits = 3)
options(width = 48)
```

# Description


This is a kaggle competition data set. \sk

There are 150,000 observations in the kaggle training data. \sk

The Y is: "Person experienced 90 days past due delinquency or worse: Y/N" \sk

Can you predict when an account is going to be delinquent!

Data can be obtained from here:   [https://www.kaggle.com/c/GiveMeSomeCredit](https://www.kaggle.com/c/GiveMeSomeCredit)


# Preprocessing

We download the data and preprocess it first. 
We split the kaggle training data into a 50% train and 50% test.
The kaggle test does not come with y! 
We made y=1 if delinquent and 0 else.

```{r}
if (!file.exists("CreditScoring.csv"))
   download.file(
    'https://github.com/ChicagoBoothML/MLClassData/raw/master/GiveMeSomeCredit/CreditScoring.csv',
    'CreditScoring.csv')

trainDf = read.csv("CreditScoring.csv")
trainDf = trainDf[,-1]
```

Add `y` as a factor and get rid of old `y = SeriousDlqin2yrs`.
```{r}
trainDf$y = as.factor(trainDf$SeriousDlqin2yrs)
trainDf = trainDf[,-1] 
```

We don't want to deal with NA's, so we drop `NumberOfDependents` and `MonthlyIncome`
```{r}
trainDf=trainDf[,-10]
trainDf=trainDf[,-5]
```

Split data into train and validation.
```{r}
set.seed(99)
n=nrow(trainDf)
ii = sample(1:n,n)
nvalid = floor(n/2)
td = trainDf[ii[1:nvalid],]
td_validation = trainDf[ii[(nvalid+1):n],]
```

\newpage

# Summary statistics

```{r}
table(trainDf$y)
```

6 to 7 % of accounts are delinquent.

For example, it looks like older people are less likely to be
delinquent.

```{r fig.height=8, fig.width=14}
plot(age~y,td,col=c("red","blue"),cex.lab=1.4)
```


\newpage

# Fit models

We fit

* logistic regression
* random forest model
* boosting

```{r message=F}
library(caret)
library(tree)
library(ranger)
library(xgboost)
```


I am creating a list to store results of  all models.
```{r}
phat.list = list() #store the test phat for the different methods here
```



## Logistic regression

We fit a logistic regression model using all variables
```{r}
lgfit = glm(y~., td, family=binomial)
print(summary(lgfit))
```

Predictions are stored for later analysis
```{r}
phat = predict(lgfit, td_validation, type="response")
phat.list$logit = matrix(phat,ncol=1) 
```



\newpage

## Random Forest

We fit random forest models for a few different settings.


```{r}
p=ncol(trainDf)-1

hyper_grid_rf <- expand.grid(
  mtry       = c(p, ceiling(sqrt(p))),
  node_size  = c(5, 10, 20)
)

# we will store phat values here
phat.list$rf = matrix(0.0, nrow(td_validation), nrow(hyper_grid_rf))  

for(i in 1:nrow(hyper_grid_rf)) {
  # train model
  rf.model <- ranger(
    formula         = y~.,
    data            = td, 
    num.trees       = 1000,
    mtry            = hyper_grid_rf$mtry[i],
    min.node.size   = hyper_grid_rf$node_size[i],
    probability     = TRUE, 
    seed            = 99
  )   
   
   # predict for random forest returns
   # a matrix of class probabilities 
   #    one column for each class and one row for each input
   # we want to record probability for class=1, 
   # which is the second column of the output
   phat = predict(rf.model, data=td_validation)$predictions[,2]
   phat.list$rf[,i]=phat
}
```

\newpage

## Boosting

We fit boosting models for a few different settings.
Remember that we need to put our data into a suitable form.

```{r}
X = Matrix::sparse.model.matrix(y ~ ., data = trainDf)[,-1]
X.train = X[ii[1:nvalid],]
Y.train = as.numeric(td$y)-1
X.validation = X[ii[(nvalid+1):n],]
Y.validation = as.numeric(td_validation$y) - 1

hyper_grid_xgb <- expand.grid(
  shrinkage = c(.01, .1),        ## controls the learning rate
  interaction.depth = c(1, 2, 4), ## tree depth
  nrounds = c(1000, 5000)         ## number of trees
)

# we will store phat values here
phat.list$boost = matrix(0.0,nrow(td_validation),nrow(hyper_grid_xgb))
```


Fitting
```{r message=F, warning=F, error=F}
for(i in 1:nrow(hyper_grid_xgb)) {
  # create parameter list
  params <- list(
    eta = hyper_grid_xgb$shrinkage[i],
    max_depth = hyper_grid_xgb$interaction.depth[i]
  )
   
  # reproducibility
  set.seed(4776)
  
  # train model
  xgb.model <- xgboost(
    data      = X.train,
    label     = Y.train,
    params    = params,
    nrounds   = hyper_grid_xgb$nrounds[i],
    objective = "binary:logistic",     # for regression models
    verbose   = 0,                     # silent
    verbosity = 0                      # silent
  )
   
  phat = predict(xgb.model, newdata=X.validation)
  phat.list$boost[,i] = phat
}
```


\newpage
# Analysis of results


## Miss-classification rate

Let us first look at miss-classification rate.


The following function computes the confusion matrix
from the vector of true class labels `y` and a vector
of estimated probabilities. The probabilities are converted
into predicted class labels using a threshold `thr`.

```{r}
# y should be 0/1
# phat are probabilities obtained by our algorithm 
# thr is the cut off value - everything above thr is classified as 1
getConfusionMatrix = function(y,phat,thr=0.5) {
   yhat = as.factor( ifelse(phat > thr, 1, 0) )
   confusionMatrix(yhat, y)
}
```

This function computes the misclassification rate
from the vector of true class labels `y` and a vector
of estimated probabilities. The probabilities are converted
into predicted class labels using a threshold `thr`.

```{r}
# y should be 0/1
# phat are probabilities obtained by our algorithm 
# thr is the cut off value - everything above thr is classified as 1
loss.misclassification.rate = function(y, phat, thr=0.5) 
   1 - getConfusionMatrix(y, phat, thr)$overall[1]
```

For **logistic regression** we have:
```{r}
cfm <- getConfusionMatrix(td_validation$y, phat.list$logit[,1], 0.5)
print(cfm, printStats = F)
cat('misclassification rate = ', 
    loss.misclassification.rate(td_validation$y, phat.list[[1]][,1], 0.5), 
    '\n')
```

\newpage

For **random forest** we have:
```{r}
nrun = nrow(hyper_grid_rf)
for(j in 1:nrun) {
  print(hyper_grid_rf[j,])
  cfm <- getConfusionMatrix(td_validation$y, phat.list[[2]][,j], 0.5)   
  print(cfm, printStats = F)
  cat('misclassification rate = ', 
      loss.misclassification.rate(td_validation$y, phat.list[[2]][,j], 0.5), 
      '\n')
}
```

\newpage

For **boosting** we have:
```{r}
nrun = nrow(hyper_grid_xgb)
for(j in 1:nrun) {
  print(hyper_grid_xgb[j,])
  cfm <- getConfusionMatrix(td_validation$y, phat.list[[3]][,j], 0.5)
  print(cfm, printStats = F)
  cat('misclassification rate = ', 
      loss.misclassification.rate(td_validation$y, phat.list[[3]][,j], 0.5), 
      '\n')
}
```


\newpage

## Deviance

The following function is used to compute the deviance of a model.

```{r}
# deviance loss function
# y should be 0/1
# phat are probabilities obtained by our algorithm 
# wht shrinks probabilities in phat towards .5 
#   this helps avoid numerical problems --- don't use log(0)!
lossf = function(y,phat,wht=0.0000001) {
   if(is.factor(y)) y = as.numeric(y)-1
   phat = (1-wht)*phat + wht*.5
   py = ifelse(y==1, phat, 1-phat)
   return(-2*sum(log(py)))
}
```

Plot test set loss --- deviance:

```{r fig.width=8, fig.height=8}
lossL = list()
nmethod = length(phat.list)
for(i in 1:nmethod) {
   nrun = ncol(phat.list[[i]])
   lvec = rep(0,nrun)
   for(j in 1:nrun) lvec[j] = lossf(td_validation$y, phat.list[[i]][,j])
   lossL[[i]]=lvec; names(lossL)[i] = names(phat.list)[i]
}
lossv = unlist(lossL)
plot(lossv, ylab="loss on Test", type="n")
nloss=0
for(i in 1:nmethod) {
   ii = nloss + 1:ncol(phat.list[[i]])
   points(ii,lossv[ii],col=i,pch=17)
   nloss = nloss + ncol(phat.list[[i]])
}
legend("topright",legend=names(phat.list),col=1:nmethod,pch=rep(17,nmethod))
```

From each method class, we choose the one that has the lowest error on the validation set.

```{r}
nmethod = length(phat.list)
phatBest = matrix(0.0,nrow(td_validation),nmethod) #pick off best from each method
colnames(phatBest) = names(phat.list)
for(i in 1:nmethod) {
   nrun = ncol(phat.list[[i]])
   lvec = rep(0,nrun)
   for(j in 1:nrun) lvec[j] = lossf(td_validation$y,phat.list[[i]][,j])
   imin = which.min(lvec)
   phatBest[,i] = phat.list[[i]][,imin]
}
```


\newpage

Each plot relates $\hat{p}$ to $y$.\sk

Going from left to right, $\hat{p}$ is from logit, random forests, and boosting.

```{r}
colnames(phatBest) = c("logit", "rf", "boost")
tempdf = data.frame(phatBest,y = td_validation$y)

par(mfrow=c(1,3))
plot(logit~y,tempdf,ylim=c(0,1),cex.lab=1.4,col=c("red","blue"))
plot(rf~y,tempdf,ylim=c(0,1),cex.lab=1.4,col=c("red","blue"))
plot(boost~y,tempdf,ylim=c(0,1),cex.lab=1.4,col=c("red","blue"))
```

Boosting and random forests both look **pretty good**!   
Both are **dramatically better than logit**!



\newpage

## Expected value of a classifier



Our **cost/benefit matrix** looks like this
```{r}
cost_benefit = matrix(c(0,-0.25,0,1), nrow=2)
print(cost_benefit)
```

If $\hat p > 0.2$, we extend credit. 

Expected values of classifiers is below:

```{r}
confMat = getConfusionMatrix(td_validation$y, phatBest[,1], 0.2)
print(confMat, printStats = F)
cat("Expected value of targeting using logistic regression = ", 
    sum(sum(as.matrix(confMat) * cost_benefit)), "\n")
```

```{r}
confMat = getConfusionMatrix(td_validation$y, phatBest[,2], 0.2)
print(confMat, printStats = F)
cat("Expected value of targeting using random forests = ", 
    sum(sum(as.matrix(confMat) * cost_benefit)), "\n")
```

```{r}
confMat = getConfusionMatrix(td_validation$y, phatBest[,3], 0.2)
print(confMat, printStats = F)
cat("Expected value of targeting using boosting = ", 
    sum(sum(as.matrix(confMat) * cost_benefit)), "\n")
```


\newpage

## ROC curves

```{r}
library(ROCR)
```

```{r}
for(i in 1:ncol(phatBest)) {
   pred = prediction(phatBest[,i], td_validation$y)
   perf = performance(pred, measure = "tpr", x.measure = "fpr")
   
   if (i == 1) {
     plot(perf, col = 1, lwd = 2,
          main= 'ROC curve', xlab='FPR', ylab='TPR', cex.lab=1)
   } else {
     plot(perf, add = T, col = i, lwd = 2)
   }
}
abline(0,1,lty=2)
legend("topleft",legend=names(phat.list),col=1:nmethod,lty=rep(1,nmethod))
```

We can also compute AUC (area under the ROC curve).

```{r}
for(i in 1:ncol(phatBest)) {
  pred = prediction(phatBest[,i], td_validation$y)
  perf <- performance(pred, measure = "auc")
  print(paste0("AUC ", names(phat.list)[i], " :: ", perf@y.values[[1]]))
}
```


\newpage 

## Lift curves

```{r}
pred = prediction(phatBest[,1], td_validation$y)
perf = performance(pred, measure = "lift", x.measure = "rpp", lwd=2)
plot(perf, col=1, ylim=c(0,5))
abline(h=1, lty=2)

for(i in 2:ncol(phatBest)) {
   pred = prediction(phatBest[,i], td_validation$y)
   perf = performance(pred, measure = "lift", x.measure = "rpp")
   plot(perf, add = T, col = i, lwd = 2)
}
legend("topright",legend=names(phat.list),col=1:nmethod,lty=rep(1,nmethod), lwd=2)
```

\newpage

## Cummulative response 

```{r fig.width=6, fig.height=6}
pred = prediction(phatBest[,1], td_validation$y)
perf = performance(pred, measure = "tpr", x.measure = "rpp")
plot(perf, col=1, ylim=c(0,1),lwd=2)
abline(h=1, lty=2)
abline(0,1,lty=2)
for(i in 2:ncol(phatBest)) {
   pred = prediction(phatBest[,i], td_validation$y)
   perf = performance(pred, measure = "tpr", x.measure = "rpp")
   plot(perf, add = T, col = i, lwd = 2)
}
legend("bottomright",legend=names(phat.list),col=1:nmethod,lty=rep(1,nmethod),lwd=2)
```

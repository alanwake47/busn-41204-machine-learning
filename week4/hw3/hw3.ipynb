{
 "cells": [
  {
   "cell_type": "raw",
   "id": "e41c0d36",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Homework 3\"\n",
    "subtitle: \"\"\n",
    "author: \"\"\n",
    "date: \"\"\n",
    "output: \n",
    "    pdf_document:\n",
    "        number_sections: true    \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea590dd",
   "metadata": {
    "name": "setup",
    "tags": [
     "remove_cell"
    ],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "knitr::opts_chunk$set(eval = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb4019",
   "metadata": {},
   "source": [
    "**Due:** end of day Saturday, February 4\n",
    "\n",
    "**Submission instructions:** Submit one write-up per group on [gradescope.com](gradescope.com). \n",
    "\n",
    "**IMPORTANT:** \n",
    "\n",
    "- Write names of everyone that worked on the assignment on the submission.\n",
    "- Specify every member of the group when submitting on Gradescope    \n",
    "  (https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70df2d2e",
   "metadata": {},
   "source": [
    "For this homework, we will be using the case _Retention Modeling at Scholastic Travel Company_. Read:\n",
    "\n",
    "- Case: Retention Modeling at Scholastic Travel Company (A);\n",
    "- Supplement: Retention Modeling at Scholastic Travel Company (B);\n",
    "\n",
    "which are available on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c225b",
   "metadata": {},
   "source": [
    "Your goal is to help David build a model for retention. \n",
    "\n",
    "The following code will get you started.\n",
    "\n",
    "# Load relevant libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aea9c40",
   "metadata": {
    "lines_to_next_cell": 2,
    "message": false,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(caret): there is no package called ‘caret’\n",
     "output_type": "error",
     "traceback": [
      "Error in library(caret): there is no package called ‘caret’\nTraceback:\n",
      "1. library(caret)"
     ]
    }
   ],
   "source": [
    "library(dplyr)\n",
    "library(caret)\n",
    "library(glmnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf4ec8f",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Here we will load the data from the CSV data file, \n",
    "examine its structure, and fix the data types \n",
    "incorrectly identified by R when importing from CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96645fc",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "STCdata_A<-read.csv('travelData.csv')\n",
    "STCdata_A<-STCdata_A[,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f690f",
   "metadata": {},
   "source": [
    "You can use the function `str` to quickly check the internal structure of an R object.\n",
    "Here we are using it to investigate type of data in each column of the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8343d5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "str(STCdata_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d416787",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Notice that some columns are identified as numerical or integer, but really the should be factors.\n",
    "\n",
    "For instance, we have that column `From.Grade`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da492d",
   "metadata": {
    "lines_to_next_cell": 0,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n_distinct(STCdata_A$From.Grade, na.rm = FALSE)   ## n_distinct is a function from dplyr package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0755cb5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "only has 11 levels. It might be a better idea to treat it as a factor instead.\n",
    "\n",
    "You can fix incorrectly classified data types as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b452baf",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "STCdata_A <- mutate_at(STCdata_A, vars(From.Grade), as.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f20e500",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We can check that indeed the column represents a factor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a56800",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "str( STCdata_A$From.Grade )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eca86bf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Fix other columns that are numeric at the moment, but could be converted to factors.\n",
    "The following line first finds numeric columns and then identifies the number of unique elements in each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7547a14",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "( unique.per.column <- sapply( dplyr::select_if(STCdata_A, is.numeric), n_distinct ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518d3f52",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let us convert every column that has less than 15 unique values into a factor.\n",
    "The following line identify names of such columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47f4ec8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "( column.names.to.factor <- names(unique.per.column)[unique.per.column < 15] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4ce4fa",
   "metadata": {},
   "source": [
    "From this, we can see that the columns \n",
    "`To.Grade`, `Is.Non.Annual.`, `Days`,\n",
    "`CRM.Segment`, `Parent.Meeting.Flag`, `MDR.High.Grade`, \n",
    "`School.Sponsor`, `NumberOfMeetingswithParents`, `SingleGradeTripFlag`\n",
    "can be converted to factors.\n",
    "We can also convert the output `Retained.in.2012.`\n",
    "\n",
    "Convert these columns into factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaa612e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "STCdata_A <- mutate_at(STCdata_A, column.names.to.factor, as.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4e0ee",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now let's take care of date columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f82fc79",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "date.columns = c('Departure.Date', 'Return.Date', 'Deposit.Date', 'Early.RPL', 'Latest.RPL', \n",
    "                 'Initial.System.Date', 'FirstMeeting', 'LastMeeting')\n",
    "STCdata_A <- mutate_at(STCdata_A, date.columns, function(x) as.Date(x, format = \"%m/%d/%Y\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1594cfa1",
   "metadata": {},
   "source": [
    "And finally we change all the character columns to factors as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcee9db2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "STCdata_A <- mutate_if(STCdata_A, is.character, as.factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6760e66",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let's see what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5275d8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "str(STCdata_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec1fdfb",
   "metadata": {},
   "source": [
    "Pretty good!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f648722",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Data preprocessing\n",
    "\n",
    "The data contains a number of columns with missing values.\n",
    "Let's investigate. \n",
    "The following tells us the number of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de982b0",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "sapply(STCdata_A, function(x) sum(is.na(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90aecb26",
   "metadata": {},
   "source": [
    "Dealing with missing values is a challenging problem, which could occupy a quarter of its own.\n",
    "The purpose of this homework is not to investigate in-depth approaches to dealing with missing values,\n",
    "but rather to investigate classification. \n",
    "For that reason, we take the following simple approach. \n",
    "\n",
    "The function `fixNAs` below fixes missing values. \n",
    "The function defines reactions:\n",
    "\n",
    " - adds a new category \"FIXED_NA\" for a missing value of a categorical/factor variable;\n",
    " - fills zero value for a missing value of a numeric variable;\n",
    " - fills \"1900-01-01\" for a missing value of a date variable.\n",
    "\n",
    "Then it loops through all columns in the dataframe, \n",
    "reads their types, and loops through all the values, \n",
    "applying the defined reaction to any missing data point. \n",
    "In addition, the function creates a surrogate dummy \n",
    "variable for each column containing at least one missing value\n",
    "(for example, `Special.Pay_surrogate`), which takes a value \n",
    "of 1 whenever the original variable (`Special.Pay`) has a\n",
    "missing value, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12790d75",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Create a custom function to fix missing values (\"NAs\") and \n",
    "# preserve the NA info as surrogate variables\n",
    "fixNAs <- function(data_frame){\n",
    "  # Define reactions to NAs\n",
    "  integer_reac <- 0\n",
    "  factor_reac <- \"FIXED_NA\"\n",
    "  character_reac <- \"FIXED_NA\"\n",
    "  date_reac <- as.Date(\"1900-01-01\")\n",
    "  \n",
    "  # Loop through columns in the data frame \n",
    "  # and depending on which class the\n",
    "  # variable is, apply the defined reaction and \n",
    "  # create a surrogate\n",
    "  \n",
    "  for (i in 1:ncol(data_frame)) {\n",
    "    if (class(data_frame[,i]) %in% c(\"numeric\",\"integer\")) {\n",
    "      if (any(is.na(data_frame[,i]))) {\n",
    "        data_frame[,paste0(colnames(data_frame)[i],\"_surrogate\")] <-\n",
    "          as.factor(ifelse(is.na(data_frame[,i]),\"1\",\"0\"))\n",
    "        data_frame[is.na(data_frame[,i]), i] <- integer_reac\n",
    "      }\n",
    "    } else\n",
    "      if (class(data_frame[,i]) %in% c(\"factor\")) {\n",
    "        if (any(is.na(data_frame[,i]))){\n",
    "          data_frame[,i]<-as.character(data_frame[,i])\n",
    "          data_frame[,paste0(colnames(data_frame)[i],\"_surrogate\")] <-\n",
    "            as.factor(ifelse(is.na(data_frame[,i]),\"1\",\"0\"))\n",
    "          data_frame[is.na(data_frame[,i]),i]<-factor_reac\n",
    "          data_frame[,i]<-as.factor(data_frame[,i])\n",
    "        }\n",
    "      } else {\n",
    "        if (class(data_frame[,i]) %in% c(\"character\")) {\n",
    "          if (any(is.na(data_frame[,i]))){\n",
    "            data_frame[,paste0(colnames(data_frame)[i],\"_surrogate\")]<-\n",
    "              as.factor(ifelse(is.na(data_frame[,i]),\"1\",\"0\"))\n",
    "            data_frame[is.na(data_frame[,i]),i]<-character_reac\n",
    "          }\n",
    "        } else {\n",
    "          if (class(data_frame[,i]) %in% c(\"Date\")) {\n",
    "            if (any(is.na(data_frame[,i]))){\n",
    "              data_frame[,paste0(colnames(data_frame)[i],\"_surrogate\")]<-\n",
    "                as.factor(ifelse(is.na(data_frame[,i]),\"1\",\"0\"))\n",
    "              data_frame[is.na(data_frame[,i]),i]<-date_reac\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "  }\n",
    "  \n",
    "  return(data_frame)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093d250",
   "metadata": {},
   "source": [
    "We apply the above defined function to our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19ca0c9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "STCdata_A<-fixNAs(STCdata_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d03d121",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We can see that the columns do not have any missing values any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f4b06e",
   "metadata": {
    "lines_to_next_cell": 2,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "any( sapply(STCdata_A, function(x) sum(is.na(x))) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a743d4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Next, we combine the rare categories. Levels that do not occur often \n",
    "during training tend not to have reliable effect estimates \n",
    "and contribute to over-fit. \n",
    "\n",
    "Let us check for rare categories in the variable `Group.State`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47cc9f7",
   "metadata": {
    "lines_to_next_cell": 2,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "table(STCdata_A$Group.State)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aaf120",
   "metadata": {},
   "source": [
    "Let us create a custom function to combine rare categories.\n",
    "The function again loops through all the columns in the dataframe,\n",
    "reads their types, and creates a table of counts \n",
    "for each level of the factor/categorical variables. All\n",
    "levels with counts less than the `mincount` are combined into \"other.\"\n",
    "The function combines rare categories into \"Other.\"+the name of the \n",
    "original variable (for example, `Other.State`).\n",
    "This function has two arguments: \n",
    "\n",
    "- the name of the dataframe; and \n",
    "- the count of observations in a category to define \"rare.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b5875",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "combinerarecategories<-function(data_frame,mincount){\n",
    "  for (i in 1:ncol(data_frame)) {\n",
    "    a<-data_frame[,i]\n",
    "    replace <- names(which(table(a) < mincount))\n",
    "    levels(a)[levels(a) %in% replace] <-\n",
    "      paste(\"Other\", colnames(data_frame)[i], sep=\".\")\n",
    "    data_frame[,i]<-a \n",
    "  }\n",
    "  return(data_frame) \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d38311",
   "metadata": {},
   "source": [
    "Let us combine categories with $<10$ values in `STCdata` into \"Other.\"\n",
    "Ultimately, it is going to depend on the person doing the analysis on what\n",
    "they decide to call ``rare''."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6231c5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "STCdata_A<-combinerarecategories(STCdata_A,10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a566920",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Let us look at `Group.State` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1e35b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "table(STCdata_A$Group.State)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9780d3b7",
   "metadata": {},
   "source": [
    "You can investigate other columns to see if everything looks fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf751c",
   "metadata": {},
   "source": [
    "# Split the data into training and testing sets\n",
    "\n",
    "This is a very important step, both conceptually and technically.\n",
    "Conceptually, because the goal of predictive modeling is not to build\n",
    "a model that fits well the data it trains on, \n",
    "but rather one that would best predict the new data.\n",
    "A test set is in this sense the best representation \n",
    "of what the \"new data\" may look like. Technically, to facilitate comparison\n",
    "between different models, we need to maintain the same IDs in\n",
    "the corresponding sets at all times.\n",
    "We will accomplishes this through two \"tricks\": \n",
    "\n",
    "- a random seed ensures that the random-number generator \n",
    "  is initialized identically in each run; and \n",
    "- the `inTrain` vector is created once and can then be applied \n",
    "  anytime the data needs to be split. \n",
    "  \n",
    "By default, the code sets 500 data points in the test set, \n",
    "and the remainder 1,889 into the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a42c23",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# set a random number generation seed to \n",
    "# ensure that the split is the same every time\n",
    "set.seed(233) \n",
    "\n",
    "inTrain <- createDataPartition(\n",
    "  y = STCdata_A$Retained.in.2012.,\n",
    "  p = 1888/2389, \n",
    "  list = FALSE)\n",
    "df.train <- STCdata_A[ inTrain,]\n",
    "df.test <- STCdata_A[ -inTrain, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf148ff",
   "metadata": {},
   "source": [
    "Let us check that both the training and test sets have a similar\n",
    "proportion of positive and negative cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf0b180",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "print('Training set proportion:')\n",
    "table(df.train$Retained.in.2012.) / nrow(df.train)\n",
    "print('Test set proportion:')\n",
    "table(df.test$Retained.in.2012.) / nrow(df.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47209a12",
   "metadata": {},
   "source": [
    "# Fitting a logistic regression model\n",
    "\n",
    "Let us fit a logistic regression model with all the variables included on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a351c21",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "lgfit.all <- glm(Retained.in.2012.~ ., \n",
    "                 data=df.train, \n",
    "                 family=\"binomial\")\n",
    "summary(lgfit.all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71206449",
   "metadata": {},
   "source": [
    "The model is overfit. It has too many insignificant variables.\n",
    "\n",
    "Let us fit a much simpler model. We will use stepwise regressions. \n",
    "\n",
    "Recall stepwise regression from BUS 41100 Applied regression course.\n",
    "See, for example, [Week 9 slides](https://maxhfarrell.com/bus41100_old/).\n",
    "You can also check Section 6.1.2 of the [ISLR](https://statlearning.com/) book.\n",
    "\n",
    "There are three approaches to running stepwise regressions: backward, forward and both.\n",
    "We need to specify criterion for inclusion/exclusion of variables.\n",
    "We will use one based on Bayesian information criteria.\n",
    "\n",
    "Observe the process of variables being added to the model,\n",
    "(labeled by \"+\" in the output), gradual expansion of the model, \n",
    "and improvement of BIC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e85762c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Start from a null model with intercept only, and add one covarite at a time until maximum BIC.\n",
    "lgfit.null <- glm(Retained.in.2012.~ 1, \n",
    "                 data=df.train, family=\"binomial\")\n",
    "\n",
    "lgfit.selected <- step(lgfit.null,                  # the starting model for our search\n",
    "                       scope=formula(lgfit.all),    # the largest possible model that we will consider.\n",
    "                       direction=\"forward\", \n",
    "                       k=log(nrow(df.train)),       # by default step() uses AIC, but by\n",
    "                                                    # multiplying log(n) on the penalty, we get BIC.\n",
    "                                                    # See ?step -> Arguments -> k\n",
    "                       trace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf12bd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "The algorithm stops once none of the 1-step expanded models lead to a lower BIC.\n",
    "\n",
    "This is the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7626f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "summary(lgfit.selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c646b3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "You can predict probabilities from this model using the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d971f25",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "phat.lgfit.selected <- predict(lgfit.selected, \n",
    "                               newdata = df.test,\n",
    "                               type = \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de7800",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "You will use these probabilities later.\n",
    "\n",
    "While we are investigating variable selection in logistic regression models, \n",
    "let us also use a more modern approach to variable selection. We will \n",
    "use the lasso. \n",
    "\n",
    "If you have not seen this in BUS 41100 Applied regression course,\n",
    "do not worry. We will provide more details in the Week 5.\n",
    "You can also check Section 6.2.2 of the [ISLR](https://statlearning.com/) book.\n",
    "\n",
    "I provide the code to fit a lasso logistic regression model.\n",
    "We find coefficients $\\beta$ that minimize the deviance\n",
    "loss plus the penalty:\n",
    "\\[\n",
    "-2\\cdot\\sum_{i=1}^n \\log p(y_i, x_i; \\beta) + \\lambda \\sum_{j=1}^p |\\beta_j|.\n",
    "\\]\n",
    "Here, $\\lambda$ is the user chosen penalty that controls the flexibility of the fit.\n",
    "\n",
    "First, we need to create a model matrix that will be used as an input to the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8dfe1",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "X <- model.matrix(formula(lgfit.all), STCdata_A)\n",
    "#need to subtract the intercept\n",
    "X <- X[,-1]\n",
    "\n",
    "X.train = X[ inTrain, ]\n",
    "X.test = X[ -inTrain, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eb9fea",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Next, we run 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0faa98",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cv.l1.lgfit <- cv.glmnet(\n",
    "  x       = X.train, \n",
    "  y       = df.train$Retained.in.2012.,\n",
    "  family  = \"binomial\", \n",
    "  alpha   = 1,   #alpha=0 gives ridge regression\n",
    "  nfolds  = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2ceb17",
   "metadata": {},
   "source": [
    "We can plot the cross-validation curve, which shows us an estimate of out-of-sample deviance \n",
    "as a function of the tuning parameter $\\lambda$.\n",
    "The x-axis represents to $-\\log(\\lambda)$. \n",
    "Therefore, on the left we have large values of $\\lambda$\n",
    "and on the right we have small values of $\\lambda$. \n",
    "At the top, you can see the number variables that were selected into the model.\n",
    "The two vertical dashed lines correspond to $\\lambda$ values that \n",
    "minimize the cross-validation error and \n",
    "the largest value of lambda such that error is within 1 standard error of the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a5b5f",
   "metadata": {
    "lines_to_next_cell": 2,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "plot(cv.l1.lgfit, sign.lambda=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e3a53",
   "metadata": {},
   "source": [
    "Let us know plot the fitted coefficients as a function of $\\lambda$.\n",
    "Note that `cv.l1.lgfit$glmnet.fit` corresponds to a fitted glmnet object for the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271977c1",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "glmnet.fit <- cv.l1.lgfit$glmnet.fit\n",
    "plot(glmnet.fit, xvar = \"lambda\")\n",
    "abline(v = log(cv.l1.lgfit$lambda.min), lty=2, col=\"red\")\n",
    "abline(v = log(cv.l1.lgfit$lambda.1se), lty=2, col=\"green\")\n",
    "legend(\"topright\", legend=c(\"min\", \"1se\"), lty=2, col=c(\"red\", \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c3d757",
   "metadata": {},
   "source": [
    "For our predictive model, we will use 1 standard error $\\lambda$.\n",
    "Below you can see the variables that are selected by the lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd13ab",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "betas <- coef(cv.l1.lgfit, s = \"lambda.1se\")\n",
    "model.1se <- which(betas[2:length(betas)]!=0)\n",
    "colnames(X[,model.1se])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f6440",
   "metadata": {},
   "source": [
    "We now use our model to predict probabilities on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3005a234",
   "metadata": {
    "lines_to_next_cell": 2,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "phat.l1.lgfit <- predict(glmnet.fit,\n",
    "                         newx = X.test,\n",
    "                         s = cv.l1.lgfit$lambda.1se,\n",
    "                         type = \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db9ef0a",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "## How well does logistic regression do?\n",
    "\n",
    "1. Create a confusion matrix for two logistic regression models build above.\n",
    "   Use probabilities `phat.lgfit.selected` and `phat.l1.lgfit` to do so.\n",
    "   \n",
    "   To solve this question, you need to make a major decision. \n",
    "   What should the cutoff or \"threshold\" for the probability be,\n",
    "   above which you will label a customer as being classified as \"retained?\"\n",
    "   In our case, the data is slightly unbalanced---about 60.72% of data points are in Class 1.\n",
    "   For very unbalanced data, we would first need to balance it (over- or under-sample).\n",
    "   In this case, the benefits of balancing are unclear, hence one can implement \n",
    "   the average probability of being retained as a cutoff. \n",
    "   \n",
    "   Predict classification using 0.6072 threshold.\n",
    "   \n",
    "   What can we see from the confusion matrices?\n",
    "   \n",
    "2. Plot ROC curves for the two classifiers and report the area under the curve.\n",
    "\n",
    "   Note that the AUC of an error-free classifier would be 100%, \n",
    "   and an AUC of a random guess would be 50%. For values in-between,\n",
    "   we can think of AUC as follows:\n",
    "   \n",
    "   - 90%+ = excellent,\n",
    "   - 80–90% = very good, \n",
    "   - 70–80% = good, \n",
    "   - 60–70% = so-so, and\n",
    "   - below 60% = not much value.\n",
    "\n",
    "3. Plot lift curves for the two classifiers.\n",
    "\n",
    "4. Create the profit curve (the amount of net profit vs the number \n",
    "   of groups targeted for promotion) for the two classifiers. \n",
    "   Suppose that the benefit of retaining a group is $100, \n",
    "   while the cost of a promotion is $40. \n",
    "   \n",
    "   How many groups should be targeted to maximize the profit?\n",
    "   \n",
    "   How would this number change as the ratio between the benefit and cost changes?\n",
    "\n",
    "   You can refer to the following code that plots a profit curve:   \n",
    "   ```{r}\n",
    "  # Function to plot a profit curve\n",
    "  #\n",
    "  # Inputs:\n",
    "  #  - benefitTP(FN/FP/TN): the net benefit for a true positive (false negative,...)\n",
    "  #      which is positive for a gain, and negative for a loss\n",
    "  #  - y: vector of true labels, which has to be labeled as \"0\" and \"1\"\n",
    "  #  - phat: vector of predicted probabilities\n",
    "  # Outputs:\n",
    "  #    the function returns the profit curve\n",
    "  \n",
    "  ProfitCurve <- function(benefitTP, benefitFN, benefitFP, benefitTN, y, phat){\n",
    "    \n",
    "    if(length(y) != length(phat)) stop(\"Length of y and phat not identical\")\n",
    "    if(length(levels(y))!=2 | levels(y)[1]!=\"0\" | levels(y)[2]!=\"1\")\n",
    "      stop(\"y should be a vector of factors, only with levels 0 and 1\")\n",
    "    \n",
    "    n <- length(y)\n",
    "    df <- data.frame(y, phat)\n",
    "    # Order phat so that we can pick the k highest groups for promotion\n",
    "    df <- df[order(df[,2], decreasing = T),]\n",
    "    TP <- 0; FP <- 0; FN <- table(y)[2]; TN <- table(y)[1]\n",
    "    \n",
    "    # Initializing the x and y coordinates of the plot\n",
    "    ratio.vec <- seq(0,n)/n\n",
    "    profit.vec <- rep(0,n+1)\n",
    "    profit.vec[1] <- FN * benefitFN + TN * benefitTN\n",
    "    \n",
    "    for(k in 1:n){ # k is the number of groups classified as \"YES\"\n",
    "      # In every round, we are picking one more group for promotion.\n",
    "      # If this group was ratained (positive), then in this round, it is classified\n",
    "      # as a \"YES\" instead of \"NO\" before. The confusion matrix is updated each round\n",
    "      # with one more TP, and one less FN. It's similar when the group was not ratained.\n",
    "      if(df[k,1]==\"1\"){TP <- TP + 1; FN <- FN - 1}\n",
    "      else{FP <- FP + 1; TN <- TN - 1}\n",
    "      #print(paste(TP, FP, TP-FP, benefitTP, benefitFP))\n",
    "      profit.vec[k+1] <- TP*benefitTP + FP*benefitFP + FN*benefitFN + TN*benefitTN\n",
    "    }\n",
    "    \n",
    "    plt <- plot(ratio.vec, profit.vec, type=\"l\", lwd=2, col=4, main=\"Profit Curve\",\n",
    "                xlab=\"Percentage of Targetted Groups\", ylab=\"Profit\")\n",
    "    abline(b=(profit.vec[n+1]-profit.vec[1]), a=profit.vec[1], lty=2) #Random guess\n",
    "    return(plt)\n",
    "  }\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ed2bb",
   "metadata": {},
   "source": [
    "5. Develop a decision tree, random forest, and a boosting model using the training data.\n",
    "   \n",
    "   Report ROC, AUC, lift, and profit curves for these models.\n",
    "   \n",
    "   How do these methods compare to the logistic regression models?\n",
    "   \n",
    "6. Investigate whether David can improve performance of his models using    \n",
    "   data he received from Emily.\n",
    "   \n",
    "   Note that in order to ensure true apples-to-apples comparison, you should\n",
    "   use the same split of data into train and test.\n",
    "   \n",
    "   You can load and merge data as follows. \n",
    "   ```{r}\n",
    "   STCdata_A <- read.csv('travelData.csv')\n",
    "   STCdata_B <- read.csv('travelData_supplement.csv')\n",
    "   STCdata_merged = merge(STCdata_A, STCdata_B, by = 'ID')\n",
    "   STCdata_merged <- STCdata_merged[,-1]\n",
    "   ```\n",
    "   Remember to fix missing values and combine rare categories.\n",
    "   \n",
    "   Comment on the improvement (or lack thereof) from incorporating the NPS data.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "name,message,tags,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

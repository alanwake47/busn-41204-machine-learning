{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd662053",
   "metadata": {},
   "source": [
    "# Week 8 Review Session\n",
    "\n",
    "TL;DR: Using pre-trained deep neural networks on the Kaggle Cats vs Dogs challenge.\n",
    "\n",
    "## Why Python, all of a sudden?\n",
    "\n",
    "- `R` -- Really good for statistics, inference (confidence intervals, hypothesis testing), linear regression, tree-based methods, biostats, data augmentation (to some extent)\n",
    "\n",
    "- `Python` -- Really good for deep learning, but some other functions may be limited (e.g. getting CIs from linear regressions requires `statsmodels`, which is poorly documented compared to `R`).\n",
    "\n",
    "- `R` good for business intelligence (visualizations and explanations), `Python` good for training large scale models where you only care about performance. If you want to explain stuff, `R` is really useful, especially when it comes to econometrics or biostats\n",
    "\n",
    "- We can keep on using `R` but ...\n",
    "\n",
    "- While `R` has `Keras` and `tensorflow` (popular deep learning libraries), it does not have `Pytorch`, another super popular DL framework. The amount of resources on `Keras` + `R` is really scarce and it would be hard to find resources after you finish this course\n",
    "\n",
    "- The techniques discussed in the guest lectures are mostly implemented in Python\n",
    "\n",
    "- Picking up Python is probably a good idea if you want to use DL for business purposes (more resources etc)\n",
    "\n",
    "\n",
    "Debating between R and Python is choosing between forks and spoons. Both are good. Both are good at different stuff. It is probably a good idea to learn how to use both and then pick the best tool for the job.\n",
    "\n",
    "## Deep Learning in Python\n",
    "\n",
    "- The research community really likes Pytorch nowadays (easier to implement complicated network structures)\n",
    "\n",
    "- But we will stick with Keras and Tensorflow (easier to use off-the-shelf models from 2016 or so)\n",
    "\n",
    "- There were some non-negligible API changes to Tensorflow and Keras due to introduction of TF2.0. Please be aware if you are trying to dig deeper on these topics (e.g. `tf.python.keras` vs `tf.keras`. They are not always the same!)\n",
    "\n",
    "## Why Use Pretrained Deep Neural Networks?\n",
    "\n",
    "- Saves time, money, space (A LOT of time, A LOT of money, and A LOT of space)\n",
    "    - In Feb 2019, OpenAI released GPT-2. Training cost ~50,000. Has 1.5 billion parameters => 11 - 12 Gigs of VRAM (GPU memory). Trained on 8 million webpages => 50 Gigs of storage for dataset at minimum (to make things faster you probably need ~80 Gigs of RAM to store the data. Reading data off disks can be a bottleneck and 80G RAM is reasonable at industrial scale). You can probably do this as an enthusiast (an instance capable of this costs ~12 per hour)\n",
    "    - Everything was reasonable until the large models attacked...\n",
    "    - GPT-3 has 175 BILLION parameters. Requires 800 Gigs of DISK SPACE just to store the model.\n",
    "    - The largest LaMDA (backbone of Google's Bard) has 135 BILLION parameters and is trained on 1.56 TRILLION words.\n",
    "    - If you are not Google/Microsoft/Meta, you probably don't have that amount of money for training.\n",
    "    - You need to trian more than once too: remember tuning? \n",
    "- In many applications, you probably don't need to do everything from scratch\n",
    "    - Word2vec contains openly available vector representations of words => sentiment analysis should be easier\n",
    "    - Networks trained on ImageNet should already be good at differentiating objects\n",
    "    - Object detection models should already be good in potential applications \n",
    "- The results are good-ish with minimum effort (which makes them amazing prototypes!)\n",
    "\n",
    "## Loading Libraries and Datasets\n",
    "There are some API changes so the top-rated code on Kaggled does not work out of the box. But that's fine. We can still do this.\n",
    "1. Download the images from [Kaggle](https://www.kaggle.com/c/dogs-vs-cats)\n",
    "2. Extract the zip and store it in the same folder/directory this .ipynb is in.\n",
    "3. Read the file names: each file name has the format \"{cat | dog}.{index}\".jpg, which gives us the label.\n",
    "4. df: a dataframe storing the sample information. First column: name of the img. Second column: cat or dog?\n",
    "5. A simple train_test_split works for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fa6fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator as IDG\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "# We are not going to bother with data augmentation (additional transformations on the images)\n",
    "img_gen = IDG()\n",
    "DATA_DIR = 'train'\n",
    "# Shamelessly inspired Kaggle because I am lazy\n",
    "filenames = os.listdir(DATA_DIR)\n",
    "categories = [f.split('.')[0] for f in filenames] # Python shorthand\n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'category': categories\n",
    "})\n",
    "\n",
    "train_df, validate_df = train_test_split(df, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ede8fb",
   "metadata": {},
   "source": [
    "## Data Generators\n",
    "Instead of storing the data on memory, you can store them on the disk and only load the images when needed.\n",
    "\n",
    "Kind of not really needed for modern hardware (ResNet50 was introduced in 2015) but could be good practice if you are dealing with large-scale data.\n",
    "\n",
    "In practice, if dataset < 60 Gigs, it is not a terrible idea to store them entirely in your memory to save disk I/O time. SSD is 10x - 100x slower than memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "810da273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 validated image filenames belonging to 2 classes.\n",
      "Found 5000 validated image filenames belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_gen = img_gen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    DATA_DIR, \n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=(224, 224), # shape required by resnet50\n",
    "    class_mode='categorical',\n",
    "    batch_size=200\n",
    ")\n",
    "\n",
    "val_data_gen = img_gen.flow_from_dataframe(\n",
    "    validate_df,\n",
    "    DATA_DIR, \n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=(224, 224),\n",
    "    class_mode='categorical',\n",
    "    batch_size=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787542e8",
   "metadata": {},
   "source": [
    "## ResNet50\n",
    "\n",
    "Using pretrained weights for ResNet50.\n",
    "\n",
    "- ResNet50: a revolutionary (really not exaggerating here) neural network structure proposed in 2015 that \"solved\" imagenet.\n",
    "- [ImageNet](https://en.wikipedia.org/wiki/ImageNet): one of the most famous benchmarks in deep learning. Cateogrizing images according to the objects in them. Kind of like training robots to do Captcha (really, really, really, really loosely speaking)\n",
    "    - It is a biased dataset (ethics of AI). See [here](https://www.wired.com/story/viral-app-labels-you-isnt-what-you-think/). The model does not perform well for all peoples, anywhere in the world.\n",
    "- Requires input images to have shape 224x224. Outputs a 1000-dimensional vector (probability that the image belongs to each of the 1000 classes in ImageNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f22c000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 21s 188ms/step\n",
      "25/25 [==============================] - 5s 196ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "resnet50 = ResNet50(weights='imagenet')\n",
    "train_features = resnet50.predict(train_data_gen)\n",
    "val_features = resnet50.predict(val_data_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd34113",
   "metadata": {},
   "source": [
    "We now have a 1000-dimensional feature vector for each observation! Let's do logistic regression!\n",
    "\n",
    "Don't laugh. The final layer of neural networks (even the deepest ones) is usually essentially doing logistic regression (if you train them to do classification)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9b4331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 1000) (5000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape, val_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5719c5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog' 'cat' 'cat' ... 'cat' 'dog' 'cat']\n",
      "There are 2448 mistakes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "lr = LR()\n",
    "lr.fit(train_features, train_df['category'])\n",
    "val_pred = lr.predict(val_features)\n",
    "print(val_pred)\n",
    "print(\"There are {} mistakes\".format(sum(val_pred != validate_df['category'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ee27e",
   "metadata": {},
   "source": [
    "Doesn't seem to work well :( The features are not scaled well.\n",
    "\n",
    "Let's try some other options & standardize the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45734c51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\boxia\\anaconda3\\envs\\gputensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR made 2442 mistakes\n",
      "Random Forests made 2444 mistakes\n",
      "5-NN made 2490 mistakes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "val_features = scaler.transform(val_features)\n",
    "\n",
    "rfc = RFC(min_samples_split = 0.01)\n",
    "knc = KNC()\n",
    "\n",
    "lr.fit(train_features, train_df['category'])\n",
    "rfc.fit(train_features, train_df['category'])\n",
    "knc.fit(train_features, train_df['category'])\n",
    "\n",
    "lr_val_pred = lr.predict(val_features)\n",
    "rfc_val_pred = rfc.predict(val_features)\n",
    "knc_val_pred = knc.predict(val_features)\n",
    "\n",
    "print(\"LR made {} mistakes\".format(sum(lr_val_pred != validate_df['category'])))\n",
    "print(\"Random Forests made {} mistakes\".format(sum(rfc_val_pred != validate_df['category'])))\n",
    "print(\"5-NN made {} mistakes\".format(sum(knc_val_pred != validate_df['category'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ec635",
   "metadata": {},
   "source": [
    "The approach unfortunately does not work well for the challenge. As future steps, you can look into [fine-tuning](https://keras.io/guides/transfer_learning/#freezing-layers-understanding-the-trainable-attribute) your ResNet50 for the Dogs vs Cats Challenge.\n",
    "\n",
    "For R user (esp for your final project), a useful approach for converting words into vectors is Word2Vec. [Here](http://www.bnosac.be/images/bnosac/blog/R_word2vec.pdf) is an excellent tutorial. Some packages use fancy pre-trained language models as backends."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“gputensorflow”",
   "language": "python",
   "name": "gputensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
